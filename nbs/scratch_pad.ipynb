{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efd81df3-9750-47cd-8e58-855d13ec4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4244d30d-7c35-46f8-9249-6fc5babd22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08c8771-2161-45e5-93b5-65205963eb1c",
   "metadata": {},
   "source": [
    "## Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f48a2cc-148c-4559-aece-0c6b9119aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8824cedf-9af7-44b6-a629-a3b0faeb6825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67e507-899a-43a1-8a9e-744e16f61e65",
   "metadata": {},
   "source": [
    "saves model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "117e3825-8cec-461b-8dab-14cc4e195fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"models/opt_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cda9617-65ea-4f03-9ba5-b9d25acf2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_name, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e317af0-8e2e-4472-a7d1-0d76398c4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70ce861b-a45d-4d43-9086-4d6a8050a769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/opt_test'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c231c9-2bfc-412e-9754-12c89466c793",
   "metadata": {},
   "source": [
    "## From Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d38e81-63e5-446c-a6d4-c00b8de68ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_at = 'reviewco/alpaca_ft/1adjrp3l_meta-llama_Llama-2-7b-hf:v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b47a45a-4ef1-469a-a895-8b7c50fbee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_class(model_path):\n",
    "    if list(model_path.glob(\"*adapter*\")):\n",
    "        return AutoPeftModelForCausalLM\n",
    "    return AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2517ab94-d1e7-4b6d-a0c9-b53cc01010d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_artifact(model_at, **model_kwargs):\n",
    "    \"\"\"Load model and tokenizer from W&B\n",
    "    If the tokenizer is not present, we load a pretrained from the hub\"\"\"\n",
    "    if not wandb.run:\n",
    "        from wandb import Api\n",
    "        api = Api()\n",
    "        artifact = api.artifact(model_at, type=\"model\")\n",
    "    else:\n",
    "        artifact = wandb.use_artifact(model_at, type=\"model\")\n",
    "    artifact_dir = Path(artifact.download())\n",
    "    \n",
    "    model = model_class(artifact_dir).from_pretrained(\n",
    "        artifact_dir,\n",
    "        **model_kwargs)\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(artifact_dir)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    except:\n",
    "        model_id = artifact.metadata.get(\"model_id\")\n",
    "        if model_id is None:\n",
    "            producer_run = artifact.logged_by()\n",
    "            config = producer_run.config\n",
    "            model_id = config.get(\"model_id\")\n",
    "        print(f\"Tokenizer not found.\\nLoading a pretrained tokenizer from the HF-hub: {model_id}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9e3747d-3c6a-4c01-b456-f77fb283e522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 1adjrp3l_meta-llama_Llama-2-7b-hf:v0, 12852.56MB. 5 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   5 of 5 files downloaded.  \n",
      "Done. 0:0:18.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb10e6323b43408d8537e76b7e2227dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer not found.\n",
      "Loading a pretrained tokenizer from the HF-hub: meta-llama/Llama-2-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LlamaForCausalLM(\n",
       "   (model): LlamaModel(\n",
       "     (embed_tokens): Embedding(32000, 4096)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x LlamaDecoderLayer(\n",
       "         (self_attn): LlamaAttention(\n",
       "           (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "           (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "           (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "           (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): LlamaMLP(\n",
       "           (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "           (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "           (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "           (act_fn): SiLUActivation()\n",
       "         )\n",
       "         (input_layernorm): LlamaRMSNorm()\n",
       "         (post_attention_layernorm): LlamaRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): LlamaRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       " ),\n",
       " LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " })"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_from_artifact(model_at, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bbaf1b-b29d-4d2a-b91a-266d5758781b",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c06de7-560f-4502-b1f5-7d3966d61694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from time import perf_counter\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "from llm_recipes.utils import parse_args, load_model_from_artifact\n",
    "from llm_recipes.data import get_alpaca_split, create_alpaca_prompt\n",
    "\n",
    "WANDB_PROJECT = \"alpaca_ft\"\n",
    "DATASET_AT = 'capecape/alpaca_ft/alpaca_gpt4_splitted:v4'\n",
    "MODEL_AT = 'reviewco/alpaca_ft/1adjrp3l_meta-llama_Llama-2-7b-hf:v0'\n",
    "\n",
    "TORCH_DTYPES = {\"bf16\":torch.bfloat16, \"fp16\":torch.float16, \"fp32\":torch.float}\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    wandb_project=WANDB_PROJECT,\n",
    "    dataset_at=DATASET_AT,\n",
    "    model_at=MODEL_AT,\n",
    "    use_cache=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bf16\",\n",
    "    num_eval_samples=10,\n",
    ")\n",
    "\n",
    "def _generate(prompt, model, tokenizer, gen_config):\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        t0 = perf_counter()\n",
    "        output = model.generate(tokenized_prompt, generation_config=gen_config)\n",
    "        generation_ids = output[0][len(tokenized_prompt[0]):]\n",
    "        num_gen_tokens = len(generation_ids)\n",
    "        time = perf_counter() - t0\n",
    "        generation = tokenizer.decode(generation_ids, skip_special_tokens=True)\n",
    "        return dict(generation=generation, generation_ids=generation_ids, time=time, num_gen_tokens=num_gen_tokens)\n",
    "    \n",
    "def prompt_table(examples, model, tokenizer, gen_config):\n",
    "    sample_out = _generate(examples[0][\"prompt\"], model, tokenizer, gen_config)\n",
    "    columns = [\"prompt\", \"label\"] + list(sample_out.keys()) + list(gen_config.to_dict().keys())\n",
    "    table = wandb.Table(columns=columns)\n",
    "    for example in tqdm(examples, leave=False):\n",
    "        prompt, label = example[\"prompt\"], example[\"output\"]\n",
    "        output = _generate(prompt, model, tokenizer, gen_config)\n",
    "        table.add_data(prompt, label, *list(output.values()), *list(gen_config.to_dict().values()))\n",
    "    return table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba613226-8726-4173-b1f9-09ef638178c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0b1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/work/llm_recipes/wandb/run-20231107_145303-sc2gxbbe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/alpaca_ft/runs/sc2gxbbe' target=\"_blank\">deep-field-256</a></strong> to <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/alpaca_ft/runs/sc2gxbbe' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/runs/sc2gxbbe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDevelop a script that prints out the Fibonacci sequence.\\n\\n### Response:\\n', 'output': 'Here is a Python script that prints out the Fibonacci sequence:\\n\\n```\\n# number of elements in the sequence\\nn = int(input(\"Enter the number of elements in the sequence: \"))\\n\\n# initial two terms\\na = 0\\nb = 1\\n\\n# define a loop to generate the sequence\\nif n <= 0:\\n    print(\"Invalid input. The number of elements must be greater than 0.\")\\n\\nelif n == 1:\\n    print(a)\\n\\nelse:\\n    print(a, b, end=\" \")  # first two elements of the sequence\\n    for i in range(3, n+1):\\n        c = a + b\\n        print(c, end=\" \")\\n        a = b\\n        b = c\\n```\\n\\nTo use, enter the number of elements you want in the sequence when prompted. The script will print out the sequence up to the specified number of elements.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 1adjrp3l_meta-llama_Llama-2-7b-hf:v0, 12852.56MB. 5 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   5 of 5 files downloaded.  \n",
      "Done. 0:0:18.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51500c2a02f468fb51eba9cfb72cef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer not found.\n",
      "Loading a pretrained tokenizer from the HF-hub: meta-llama/Llama-2-7b-hf\n"
     ]
    }
   ],
   "source": [
    "# parse_args(config)\n",
    "wandb.init(project=config.wandb_project, job_type=\"eval\", config=config)\n",
    "\n",
    "_, eval_dataset = get_alpaca_split(dataset_at=config.dataset_at)\n",
    "\n",
    "model, tokenizer, artifact_dir = load_model_from_artifact(\n",
    "    config.model_at, \n",
    "    use_cache=config.use_cache,\n",
    "    device_map=config.device_map,\n",
    "    torch_dtype=TORCH_DTYPES[config.torch_dtype],\n",
    ")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca70876b-67c5-43d7-a673-381106e023c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference stuff\n",
    "gen_config = GenerationConfig.from_pretrained(artifact_dir, max_new_tokens=256)\n",
    "config.gen_config = gen_config  # we save this \n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d79e7d79-3bc4-4998-970f-be454240f184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_config.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e256f750-0ac8-40f0-8071-29c5d61e36bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on your model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Running inference on your model!\")\n",
    "eval_samples = eval_dataset[:config.num_eval_samples]\n",
    "table = prompt_table(eval_samples, model=model, tokenizer=tokenizer, gen_config=gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b542ae-5bb1-4aae-8626-c039da5b0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"eval_predictions\":table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "689e25fc-9296-43bf-bd16-454fbe6b3cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-field-256</strong> at: <a href='https://wandb.ai/capecape/alpaca_ft/runs/sc2gxbbe' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/runs/sc2gxbbe</a><br/> View job at <a href='https://wandb.ai/capecape/alpaca_ft/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzQ1MDUyNA==/version_details/v0' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzQ1MDUyNA==/version_details/v0</a><br/>Synced 7 W&B file(s), 0 media file(s), 8 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_145303-sc2gxbbe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "959c2d39-42b7-4ead-b338-74492ac00b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=table.data, columns=table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b7168-b08f-4416-a122-ae53a6ba3e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421e2b6-1f0e-42f4-a42f-3fcbae539372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bac6183f-2b26-47fc-9e2f-c532f866097f",
   "metadata": {},
   "source": [
    "## GPT3.5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62ba2f73-32dd-4c95-b0d5-e7aa73983bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import wandb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63292400-6b23-4838-937d-5a60d9ee001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_config = SimpleNamespace(\n",
    "    max_tokens=256,\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "488cc65e-412c-4747-aeb0-6dbe9c677f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJECT = \"alpaca_ft\"\n",
    "ENTITY = \"reviewco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9928461-f464-4545-b744-2a80c9721208",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_df = pd.read_csv(\"gpt35_test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a863038-9215-47ce-9473-9af444d8a52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "      <td>Based on the elevated levels of white blood ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "      <td>Estamos emocionados de trabajar contigo en est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>Here are five potential threats to digital sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>One potential area for improvement for my favo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>The purpose behind drug testing in professiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Below is an instruction that describes a task,...   \n",
       "1  Below is an instruction that describes a task,...   \n",
       "2  Below is an instruction that describes a task....   \n",
       "3  Below is an instruction that describes a task....   \n",
       "4  Below is an instruction that describes a task....   \n",
       "\n",
       "                                          generation  \n",
       "0  Based on the elevated levels of white blood ce...  \n",
       "1  Estamos emocionados de trabajar contigo en est...  \n",
       "2  Here are five potential threats to digital sec...  \n",
       "3  One potential area for improvement for my favo...  \n",
       "4  The purpose behind drug testing in professiona...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fef32696-198a-42b3-95fc-c9e8953a3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_table = wandb.Table(dataframe=gpt35_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4e1d132-c14f-4dae-882a-19349c56e87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m (\u001b[33mreviewco\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/work/llm_recipes/wandb/run-20231107_222408-n299aec1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reviewco/alpaca_ft/runs/n299aec1' target=\"_blank\">smooth-gorge-18</a></strong> to <a href='https://wandb.ai/reviewco/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reviewco/alpaca_ft' target=\"_blank\">https://wandb.ai/reviewco/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reviewco/alpaca_ft/runs/n299aec1' target=\"_blank\">https://wandb.ai/reviewco/alpaca_ft/runs/n299aec1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-gorge-18</strong> at: <a href='https://wandb.ai/reviewco/alpaca_ft/runs/n299aec1' target=\"_blank\">https://wandb.ai/reviewco/alpaca_ft/runs/n299aec1</a><br/> View job at <a href='https://wandb.ai/reviewco/alpaca_ft/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzU1MDUxMA==/version_details/v0' target=\"_blank\">https://wandb.ai/reviewco/alpaca_ft/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzU1MDUxMA==/version_details/v0</a><br/>Synced 6 W&B file(s), 1 media file(s), 6 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_222408-n299aec1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=WANDB_PROJECT, entity=ENTITY, job_type=\"inference\", tags=[\"gpt-3.5\"], config=gpt35_config):\n",
    "    wandb.log({\"gpt35_results\":gpt35_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2942c8b1-ea95-4730-90d8-265065f46e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb6b09a3-fafb-48e6-8ffa-cf2b1dbeae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ab31343-9eb0-4a71-960f-1f58e4968b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e91ed-f59e-49eb-9063-8178e8282df6",
   "metadata": {},
   "source": [
    "## OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9d0755-196e-4f0b-a86e-04dbfa23f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d9affd-98cd-40fc-ac10-5657d690ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"facebook/opt-350m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63534b29-093d-4b15-8481-23216ca01b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77546bd4-a7ae-4963-ac1e-f50d812304a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d997562-24c2-495d-b530-1901cb2301be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_mod(model, module_name=\"layers\"):\n",
    "    for name, mod in model.named_modules():\n",
    "        if name.endswith(module_name):\n",
    "            return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0436e1f-7a82-461e-85dc-65963f18b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = _find_mod(model, \"embed_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "051921da-caa4-460f-9dc3-403024ef4b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50272, 512, padding_idx=1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3e9fc-6b05-4b61-a43f-dce0982b659e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
