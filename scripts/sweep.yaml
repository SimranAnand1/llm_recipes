program: train_hf.py
method: grid
entity: capecape
project: alpaca_ft

parameters:
  model_id: 
    value: 'meta-llama/Llama-2-7b-hf'
  n_freeze:
    value: 24
  gradient_checkpointing:
    values: ['True', 'False']
  use_lora:
    values: ['True', 'False']
  batch_size:
    values: [1,2,4,8,16,32]
  max_seq_len:
    values: [512, 1024]
  max_steps:
    value: 100
  evaluate:
    value: False
