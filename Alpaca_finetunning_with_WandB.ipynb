{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
   "metadata": {},
   "source": [
    "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
    "In this notebooks you will learn how to finetune a pretrained LLama model on an Instruction dataset. We will use an updated version of the Alpaca dataset that, instead of davinci-003 (GPT3) generations uses GPT4 to get an even better instruction dataset! More details on the [official repo page](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)\n",
    "\n",
    "> This notebook requires a A100/A10 GPU with at least 24GB of memory. You could tweak the params down and run on a T4 but it would take very long time\n",
    "\n",
    "This notebooks has a companion project and [report](wandb.me/alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ef319f-bf26-4192-8951-8d536181ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/ubuntu/.local/lib/python3.8/site-packages (0.15.12)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.8/site-packages (4.34.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: pathtools in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (45.2.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (3.1.38)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/lib/python3/dist-packages (from wandb) (5.5.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/lib/python3/dist-packages (from wandb) (1.4.3)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (4.8.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (1.32.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7804f904-5746-4530-867d-c766f4501dea",
   "metadata": {},
   "source": [
    "## Prepare your Instruction Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd7517-70d9-4dee-9f92-1a2891caf385",
   "metadata": {},
   "source": [
    "An Instruction dataset is a list of instructions/outputs pairs that are relevant to your own domain. For instance it could be question and answers from an specific domain, problems and solution for a technical domain, or just instruction and outputs. A typical example is \"Write me a Python script to read a jsonL file and print the first 5 lines\" and the model would output something like:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "fname = \"my_file.json\"\n",
    "\n",
    "# read file from fname\n",
    "with open(fname, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[0:5])\n",
    "```\n",
    "\n",
    "So let's explore how one could do this?\n",
    "\n",
    "After grabbing a finetuned model and curated your own dataset, how do I create a dataset that has the right format to fine tune a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c0a5-f481-4364-880d-10c254388987",
   "metadata": {},
   "source": [
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0665a80-6137-4a61-a3da-93bde606df04",
   "metadata": {},
   "source": [
    "Let's load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_file = \"alpaca_gpt4_data.json\"\n",
    "\n",
    "with open(dataset_file, \"r\") as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9618cd92-acdd-471b-9521-d55c38af8040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [{'instruction': 'Give three tips for staying healthy.',\n",
       "   'input': '',\n",
       "   'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.'},\n",
       "  {'instruction': 'What are the three primary colors?',\n",
       "   'input': '',\n",
       "   'output': 'The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).'},\n",
       "  {'instruction': 'Describe the structure of an atom.',\n",
       "   'input': '',\n",
       "   'output': \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\"}],\n",
       " 52002)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alpaca), alpaca[0:3], len(alpaca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596e369-56aa-4721-9271-6686eed8fb35",
   "metadata": {},
   "source": [
    "So the dataset has instruction and outputs. The model is trained to predict the next token, so one option would be just to concat both, and train on that. We ideally format the prompt in a way that we make explicit where is the input and output. Let's log the dataset to W&B so we keep everything organised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9786466-4012-4cc4-8f9a-e673c74aa965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/cape/llm_recipes/wandb/run-20231020_171106-gc05g4iu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/alpaca_ft/runs/gc05g4iu' target=\"_blank\">youthful-fog-99</a></strong> to <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/alpaca_ft/runs/gc05g4iu' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/runs/gc05g4iu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44265fae9e3d44aebc60a417bb1f6e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='46.050 MB of 46.050 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-fog-99</strong> at: <a href='https://wandb.ai/capecape/alpaca_ft/runs/gc05g4iu' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/runs/gc05g4iu</a><br/> View job at <a href='https://wandb.ai/capecape/alpaca_ft/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwODQ0NzcwNQ==/version_details/v3' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwODQ0NzcwNQ==/version_details/v3</a><br/>Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231020_171106-gc05g4iu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# log to wandb\n",
    "with wandb.init(project=\"alpaca_ft\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"alpaca_gpt4\", \n",
    "        type=\"dataset\",\n",
    "        description=\"A GPT4 generated Alpaca like dataset for instruction finetunning\",\n",
    "        metadata={\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
    "    )\n",
    "    at.add_file(dataset_file)\n",
    "\n",
    "    # log as a table\n",
    "    table = wandb.Table(columns=list(alpaca[0].keys()))\n",
    "    for row in alpaca:\n",
    "        table.add_data(*row.values())\n",
    "    wandb.log({\"alpaca_gpt4_table\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f98ce99-704b-469f-b490-04abe3bfc7c7",
   "metadata": {},
   "source": [
    "### Train/Eval Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dfa1958-c58d-4b40-a7d9-5e0cc6d2abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(alpaca)  # this could also be a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5492465c-c1b8-4f04-a154-36ce3bcb6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = alpaca[:-1000]\n",
    "eval_dataset = alpaca[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc9e43-55b5-4a7b-902f-b8a3b82dbf06",
   "metadata": {},
   "source": [
    "We should save the split to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdbc0fd0-de6c-447d-abb9-3176c6ceeaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/cape/llm_recipes/wandb/run-20231020_171118-md1t86jr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/alpaca_ft/runs/md1t86jr' target=\"_blank\">elated-pine-100</a></strong> to <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/alpaca_ft/runs/md1t86jr' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/runs/md1t86jr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-pine-100</strong> at: <a href='https://wandb.ai/capecape/alpaca_ft/runs/md1t86jr' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/runs/md1t86jr</a><br/> View job at <a href='https://wandb.ai/capecape/alpaca_ft/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwODQ0NzcwNQ==/version_details/v4' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwODQ0NzcwNQ==/version_details/v4</a><br/>Synced 5 W&B file(s), 2 media file(s), 7 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231020_171118-md1t86jr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "eval_df = pd.DataFrame(eval_dataset)\n",
    "\n",
    "train_table = wandb.Table(dataframe=train_df)\n",
    "eval_table  = wandb.Table(dataframe=eval_df)\n",
    "\n",
    "train_df.to_json(\"alpaca_gpt4_train.jsonl\", orient='records', lines=True)\n",
    "eval_df.to_json(\"alpaca_gpt4_eval.jsonl\", orient='records', lines=True)\n",
    "\n",
    "with wandb.init(project=\"alpaca_ft\", job_type=\"split_data\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"alpaca_gpt4_splitted\", \n",
    "        type=\"dataset\",\n",
    "        description=\"A GPT4 generated Alpaca like dataset for instruction finetunning\",\n",
    "        metadata={\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
    "    )\n",
    "    at.add_file(\"alpaca_gpt4_train.jsonl\")\n",
    "    at.add_file(\"alpaca_gpt4_eval.jsonl\")\n",
    "    wandb.log_artifact(at)\n",
    "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54",
   "metadata": {},
   "source": [
    "Let's log the dataset also as a table so we can inspect it on the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024aec96-40fb-4417-8e64-060a301b0f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Discuss the history and usage of the barometer.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = alpaca[0]\n",
    "print(prompt_no_input(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037",
   "metadata": {},
   "source": [
    "Some other instruction have some context in the `input` variable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e42d41-a95a-41de-a3cc-1461d9e10ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Discuss the history and usage of the barometer.',\n",
       " 'input': '',\n",
       " 'output': 'A barometer is an instrument that measures atmospheric pressure, which is used to forecast changes in weather and other atmospheric conditions. The barometer has had a long and fascinating history, with many notable figures contributing to its development over the years.\\n\\nThe concept of atmospheric pressure was first explored by the scientist Evangelista Torricelli in the 17th century. He determined that air had weight and could be measured. In 1643, Torricelli designed and built the first barometer, which consisted of a long glass tube filled with mercury that was inverted into a container of mercury. As the atmospheric pressure increased, it would force the mercury in the tube to rise, and as the pressure decreased, the mercury would fall.\\n\\nThis type of barometer is known as a mercury barometer, and it remained the most common type of barometer for several centuries. However, other types of barometers were developed over time, including the aneroid barometer, which uses the expansion and contraction of a sealed container to measure changes in pressure.\\n\\nBarometers are used to predict changes in weather because changes in atmospheric pressure can be used to forecast changes in weather conditions. For example, a rising barometric pressure usually indicates that fair weather is on the way, while falling pressure may indicate that a storm is approaching.\\n\\nIn addition to weather forecasting, barometers are used in a number of other fields. They are used in aviation to help determine altitude and in oceanography to measure changes in water pressure at different depths. Barometers are also used in scientific research to study atmospheric conditions and to calibrate other instruments.\\n\\nToday, barometers continue to be a valuable tool for weather prediction and scientific research. With advances in technology, digital barometers have become more accurate and easier to use. However, the basic principles behind the barometer have remained largely the same since its invention over 300 years ago.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b795343f-0356-4689-8bc6-9ac650716c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Debate the pros and cons of the proposed soda tax.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = alpaca[232]\n",
    "print(prompt_input(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a",
   "metadata": {},
   "source": [
    "> But you are leaving the output out!!! Yes, but we can just concat that afterwards. Let's deal with the prompt now, we can add the output later with the right amount of padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f",
   "metadata": {},
   "source": [
    "And the refactored function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4038166-085c-4b10-a56a-2bee0bd62436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34cc42-c38c-4e6f-bfb6-0f32d48aef5d",
   "metadata": {},
   "source": [
    "## Why are we doing all this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31dd88-e90e-4697-b354-b39eed0fab3c",
   "metadata": {},
   "source": [
    "Because we need to tokenize this dataset in a very particular way, if we want the model to learn to predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41703ce9-a22d-4245-9f6c-a424afd9ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts = [create_prompt(row) for row in train_dataset]\n",
    "eval_prompts = [create_prompt(row) for row in eval_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Discuss the history and usage of the barometer.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69153b-eb20-4f6d-afba-5b737d36320b",
   "metadata": {},
   "source": [
    "We need to process the targets and add the End Of String token (EOS) to the results. For LLama this is: `\"</s>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_eos(ds):\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    return [f\"{row['output']}{EOS_TOKEN}\" for row in ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc5b21e8-3d5b-4964-be7b-faff5038f7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A barometer is an instrument that measures atmospheric pressure, which is used to forecast changes in weather and other atmospheric conditions. The barometer has had a long and fascinating history, with many notable figures contributing to its development over the years.\\n\\nThe concept of atmospheric pressure was first explored by the scientist Evangelista Torricelli in the 17th century. He determined that air had weight and could be measured. In 1643, Torricelli designed and built the first barometer, which consisted of a long glass tube filled with mercury that was inverted into a container of mercury. As the atmospheric pressure increased, it would force the mercury in the tube to rise, and as the pressure decreased, the mercury would fall.\\n\\nThis type of barometer is known as a mercury barometer, and it remained the most common type of barometer for several centuries. However, other types of barometers were developed over time, including the aneroid barometer, which uses the expansion and contraction of a sealed container to measure changes in pressure.\\n\\nBarometers are used to predict changes in weather because changes in atmospheric pressure can be used to forecast changes in weather conditions. For example, a rising barometric pressure usually indicates that fair weather is on the way, while falling pressure may indicate that a storm is approaching.\\n\\nIn addition to weather forecasting, barometers are used in a number of other fields. They are used in aviation to help determine altitude and in oceanography to measure changes in water pressure at different depths. Barometers are also used in scientific research to study atmospheric conditions and to calibrate other instruments.\\n\\nToday, barometers continue to be a valuable tool for weather prediction and scientific research. With advances in technology, digital barometers have become more accurate and easier to use. However, the basic principles behind the barometer have remained largely the same since its invention over 300 years ago.</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outputs = pad_eos(train_dataset)\n",
    "eval_outputs = pad_eos(eval_dataset)\n",
    "train_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42190f2-20cf-4960-866b-ccb7700b5b20",
   "metadata": {},
   "source": [
    "Cool! but why we have everything separated? Let's sore the \"final\" version on a variable called `examples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdea7bc4-1ec3-451e-ab00-549aa2056800",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(train_prompts, train_outputs)]\n",
    "eval_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(eval_prompts, eval_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad92d95-23d5-474c-9271-59948d5dcbb0",
   "metadata": {},
   "source": [
    "This is what the model need to see and learn =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Discuss the history and usage of the barometer.\n",
      "\n",
      "### Response:\n",
      "A barometer is an instrument that measures atmospheric pressure, which is used to forecast changes in weather and other atmospheric conditions. The barometer has had a long and fascinating history, with many notable figures contributing to its development over the years.\n",
      "\n",
      "The concept of atmospheric pressure was first explored by the scientist Evangelista Torricelli in the 17th century. He determined that air had weight and could be measured. In 1643, Torricelli designed and built the first barometer, which consisted of a long glass tube filled with mercury that was inverted into a container of mercury. As the atmospheric pressure increased, it would force the mercury in the tube to rise, and as the pressure decreased, the mercury would fall.\n",
      "\n",
      "This type of barometer is known as a mercury barometer, and it remained the most common type of barometer for several centuries. However, other types of barometers were developed over time, including the aneroid barometer, which uses the expansion and contraction of a sealed container to measure changes in pressure.\n",
      "\n",
      "Barometers are used to predict changes in weather because changes in atmospheric pressure can be used to forecast changes in weather conditions. For example, a rising barometric pressure usually indicates that fair weather is on the way, while falling pressure may indicate that a storm is approaching.\n",
      "\n",
      "In addition to weather forecasting, barometers are used in a number of other fields. They are used in aviation to help determine altitude and in oceanography to measure changes in water pressure at different depths. Barometers are also used in scientific research to study atmospheric conditions and to calibrate other instruments.\n",
      "\n",
      "Today, barometers continue to be a valuable tool for weather prediction and scientific research. With advances in technology, digital barometers have become more accurate and easier to use. However, the basic principles behind the barometer have remained largely the same since its invention over 300 years ago.</s>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"example\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd",
   "metadata": {},
   "source": [
    "## Converting text to numbers: Tokenizer\n",
    "We need to convert the dataset into tokens, you can quickly do this with the workhorse of the transformers library, the Tokenizer! This function does a lot of heavy lifting besides tokenizing the text. \n",
    "\n",
    "- It tokenizes the text\n",
    "- Converts the outputs to PyTorch tensors\n",
    "- Pads the inputs to match length and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "337fedfd-e238-4e86-a96b-24dfeed11f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1619, 15729, 526, 2675, 4549, 29991]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"My experiments are going strong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20c69466-f7e6-45b8-a167-718c80cedc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1619, 15729, 526, 2675, 4549, 29991, 2, 2, 2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"My experiments are going strong!\", padding='max_length', max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb1e5e29-254a-4dbf-ac02-ea6bb2a16e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1619, 15729,   526,  2675,  4549, 29991,     2,     2,     2]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"My experiments are going strong!\", \n",
    "                 padding='max_length', \n",
    "                 max_length=10,\n",
    "                 return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a89a61d-34b7-4a56-b1d8-98ebcf3384d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  1619, 15729,   526,  2675,  4549, 29991,     2,     2,     2],\n",
       "        [    1,   306,  5360,   365,  5288,   294,     2,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"My experiments are going strong!\", \n",
    "           \"I love Llamas\"], \n",
    "          padding='max_length', \n",
    "          # padding='longest',\n",
    "          max_length=10,\n",
    "          return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58586813-7918-4578-9583-df14c5a6a6ad",
   "metadata": {},
   "source": [
    "### Packing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316d169-4292-41aa-a42d-cd49620a881c",
   "metadata": {},
   "source": [
    "We will pack multiple short examples into a longer chunk, so we can train more efficiently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a537da60-db69-42a7-8c66-0ea3756c2847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pack(dataset, max_seq_len=1024):\n",
    "    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]\n",
    "    \n",
    "    all_token_ids = []\n",
    "    for tokenized_input in tkds_ids:\n",
    "        all_token_ids.extend(tokenized_input)# + [tokenizer.eos_token_id])\n",
    "    \n",
    "    print(f\"Total number of tokens: {len(all_token_ids)}\")\n",
    "    packed_ds = []\n",
    "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
    "        input_ids = all_token_ids[i : i + max_seq_len+1]\n",
    "        if len(input_ids) == (max_seq_len+1):\n",
    "            packed_ds.append({\"input_ids\": input_ids[:-1], \"labels\": input_ids[1:]})  # this shift is not needed if using the model.loss\n",
    "    return packed_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6df54e-f6bc-4e56-aec2-014a19fc654c",
   "metadata": {},
   "source": [
    "The main idea here is that the instruction/output samples are short, so let's concatenate a bunch of them together separated by the `EOS` token. We can also pre-tokenize and pre-pack the dataset and make everything faster!  If we define a `max_seq_len = 1024` the code to pack would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c78b4e9-2c7f-4aa1-b738-be04bc55b06b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 11486294\n",
      "Total number of tokens: 230082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11206"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_packed = pack(train_dataset)\n",
    "eval_ds_packed = pack(eval_dataset)\n",
    "len(train_ds_packed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df389230-911c-447c-a177-a18c22837020",
   "metadata": {},
   "source": [
    "Doing so, we end up with a little more than 11k sequences of lenght 1024. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43713b3-9f65-42a6-8338-ab0601e5f476",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "As we are training for completion, the labels (or targets) will be the inputs shifted by one. We are going to train with regular Cross Entropy and predict the next token on this packed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f342873a-29a3-4ed1-8b59-9e7f7f2a4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 8  # I have an A100 GPU with 40GB of RAM 😎\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator, # we don't need any special collator 😎\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds_packed,\n",
    "    batch_size=batch_size//4,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ccaf4-dfa1-4daa-9a6f-244c8cda7818",
   "metadata": {},
   "source": [
    "It is always a good idea to check how does a batch looks like, you can quickly do this by sampling from the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffced7ec-bd1b-42b0-be64-04f3fae5df84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 13866,   338,  ...,   571,   292,   901],\n",
       "         [ 1907,   363,  6901,  ...,   434,  1009, 14433],\n",
       "         [15020,   278, 14979,  ...,  4160, 29915,   848],\n",
       "         ...,\n",
       "         [ 5192, 29899,  7864,  ...,  1048,  2020, 10832],\n",
       "         [  508,   367,  7795,  ...,   363,   385,  7395],\n",
       "         [  297,  3630,  9327,  ...,  6680, 29901,   319]]),\n",
       " 'labels': tensor([[13866,   338,   385,  ...,   292,   901, 28602],\n",
       "         [  363,  6901,  5870,  ...,  1009, 14433, 29892],\n",
       "         [  278, 14979, 23435,  ..., 29915,   848, 29889],\n",
       "         ...,\n",
       "         [29899,  7864,  3262,  ...,  2020, 10832,  1862],\n",
       "         [  367,  7795,  5611,  ...,   385,  7395,  3236],\n",
       "         [ 3630,  9327, 29901,  ..., 29901,   319,  6483]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(train_dataloader))\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d15d30-58d2-465d-a9f4-be0eef2ea06b",
   "metadata": {},
   "source": [
    "We can alos decode the batch just to be super sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28e18ad2-c141-4902-a5a4-24a1e743be35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDiscuss the history and usage of the barometer.\\n\\n### Response:\\nA barometer is an instrument that measures atmospheric pres'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(b[\"input_ids\"][0])[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0a46e93-7ec2-4365-8e50-be7bef873436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDiscuss the history and usage of the barometer.\\n\\n### Response:\\nA barometer is an instrument that measures atmospheric pressure'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(b[\"labels\"][0])[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757e458-14fd-4dd3-861f-efad04dce787",
   "metadata": {},
   "source": [
    "I like storing all my hyperparameters into a `SimpleNamespace`, it's like a dict but with .dot attribute access. Then I can access my batch size by doing config.batch_size instead of config[\"batch_size\"]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6925a62e-d85e-4c86-8867-bee3a180fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    model_id='meta-llama/Llama-2-7b-hf',\n",
    "    dataset_name=\"alpaca-gpt4\",\n",
    "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
    "    n_freeze=24,  # How many layers we don't train, LLama 7B has 32.\n",
    "    lr=2e-4,\n",
    "    n_eval_samples=10, # How many samples to generate on validation\n",
    "    max_seq_len=1024, # Lenght of the sequences to pack\n",
    "    epochs=1,  # we do one pass over the dataset, we could actually do more\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
    "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training  \n",
    "    epoch_sz=len(train_dataloader) // gradient_accumulation_steps ,  # the theorical epoch size, here it's just the steps\n",
    "    eval_every=500,  # every now and then we want to sample from the model\n",
    "    log_model=False,  # upload the model to W&B?\n",
    "    mom=0.9, # optim param\n",
    "    gradient_checkpointing = True,  # saves even more memory\n",
    "    freeze_embed = True,  # why train this? let's keep them frozen ❄️\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd0229-5e4c-4bb5-827b-309bdbb351df",
   "metadata": {},
   "source": [
    "We first get a pretrained model with some configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfe737e8b4049a4af5c8c3a9aae771e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map=0,\n",
    "    # use_flash_attention_2=True,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 6738.42M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57",
   "metadata": {},
   "source": [
    "Training the full models is expensive, but if you have a GPU that can fit the full model, you can skip this part. Let's just train the last 8 layers of the model (Llama2-7B has 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_freeze = 24\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "if config.freeze_embed:\n",
    "    model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284599f6-ba88-4172-a311-8e618f716b30",
   "metadata": {},
   "source": [
    "and you can also use gradient checkpointing to save even more (this makes training slower, how much it will depend on your particular configuration). There is a [nice article](https://huggingface.co/docs/transformers/v4.18.0/en/performance) on the Huggingface website about how to fit large models on memory, I encourage you to check it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "750ce64e-0088-4ca8-9bc9-60037e7110d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save more memory\n",
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6232ce7a-b847-45c8-8ff7-e36249e7a060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 1750.14M\n"
     ]
    }
   ],
   "source": [
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc92663c-95a4-4ecc-a9af-7a68d9648271",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "We setup the standard optimization stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5374c44d-517b-42a0-ade6-297c0a5d18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps=config.epoch_sz,\n",
    "    num_warmup_steps=config.epoch_sz // 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5efdd402-c851-47a5-9134-5b47b7d118e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24440392-9837-4cbb-873a-372f9f5aca20",
   "metadata": {},
   "source": [
    "## Testing during training\n",
    "\n",
    "We are almost there, let's create a simple function to sample from the model now and then, to visualy see what the models is outputting!\n",
    "Let's wrap the model.generate method for simplicity. You can grab the defaults sampling parameters from the GenerationConfig and passing the corresponding model_id. This will grab you the defaults for parameters like temperature, top p, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0a92fe7-3f9e-43e6-80b0-adbbd8b480ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=256,\n",
    "    gen_config=gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ec41718-52c6-4335-a534-2790d03ba069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44601edc-db5b-40ea-bb74-9591ea4e7e50",
   "metadata": {},
   "source": [
    "LoL 🤷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4e39c49-ccb1-4fe6-aa30-988ea5583b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Add five interesting adjectives to this sentence.\n",
      "\n",
      "### Input:\n",
      "The sky was different colors tonight.\n",
      "\n",
      "### Response:\n",
      "The sky was different colors tonight. It was purple, red, blue, green, and orange.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = eval_dataset[14][\"prompt\"]\n",
    "print(prompt + generate(prompt, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567920c0-005d-419d-98ab-4d797b243300",
   "metadata": {},
   "source": [
    "We can log a Table with those results to the project every X steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bac25c48-cb18-4560-b05c-1748e7d1adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "def prompt_table(examples, log=False):\n",
    "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"output\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
    "    for example in progress_bar(examples):\n",
    "        prompt, gpt4_output = example[\"prompt\"], example[\"output\"]\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, gpt4_output, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    if log:\n",
    "        wandb.log({\"predictions\":table})\n",
    "    return table\n",
    "\n",
    "def to_gpu(tensor_dict):\n",
    "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
    "\n",
    "class Accuracy:\n",
    "    \"A simple Accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.tp = 0.\n",
    "    def update(self, logits, labels):\n",
    "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
    "        tp = (logits == labels).sum()\n",
    "        self.count += len(logits)\n",
    "        self.tp += tp\n",
    "        return tp / len(logits)\n",
    "    def compute(self):\n",
    "        return self.tp / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52fbc09-5d65-4265-abce-adda01d85584",
   "metadata": {},
   "source": [
    "You can also quickly add validation if you feel so, the table can be also created at this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6891b2c0-f22e-4647-9ac2-e07a994f3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval();\n",
    "    eval_acc = Accuracy()\n",
    "    for step, batch in enumerate(progress_bar(eval_dataloader)):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
    "        eval_acc.update(out.logits, batch[\"labels\"])\n",
    "    # we log results at the end\n",
    "    wandb.log({\"eval_loss\": loss.item(),\n",
    "               \"eval_accuracy\": eval_acc.compute()})\n",
    "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
    "    model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37364791-987e-4e81-9621-3094ed5bf86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
    "    \"\"\"Save the model to wandb as an artifact\n",
    "    Args:\n",
    "        model (nn.Module): Model to save.\n",
    "        model_name (str): Name of the model.\n",
    "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
    "    \"\"\"\n",
    "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
    "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
    "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(file_name, safe_serialization=True)\n",
    "    # save tokenizer for easy inference\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "    if log_model:\n",
    "        at = wandb.Artifact(model_name, type=\"model\")\n",
    "        at.add_dir(file_name)\n",
    "        wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93834c0b-15e1-4e43-8535-dbb9f36af29d",
   "metadata": {},
   "source": [
    "Let's define a loop that compute evaluation and logs a Table with model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90177c-0c5a-48e7-9a39-2b9e67794814",
   "metadata": {},
   "source": [
    "## The actual Loop\n",
    "It's actually nothing fancy, and very short! It has:\n",
    "- Gradient accumulation and gradient scaling\n",
    "- sampling and model checkpoint saving (this trains very fast, no need to save multiple checkpoints)\n",
    "- We compute token accuracy, better metric than loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9342333d-6a48-42e4-9b4e-88e42bb35c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2324b74427d847509855e915aea5fc6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0111134784671271, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/cape/llm_recipes/wandb/run-20231020_171138-s2juaxiw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/alpaca_ft/runs/s2juaxiw' target=\"_blank\">rare-paper-101</a></strong> to <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/alpaca_ft/runs/s2juaxiw' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/runs/s2juaxiw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='464' class='' max='1401' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      33.12% [464/1401 22:20&lt;45:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='112' class='' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [112/112 01:13&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='10' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [10/10 01:21&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-e42cce70b373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# we can log the metrics to W&B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         wandb.log({\"loss\": loss.item() * config.gradient_accumulation_steps,\n\u001b[0m\u001b[1;32m     16\u001b[0m                    \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                    \"lr\": scheduler.get_last_lr()[0]})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           tags=[\"baseline\"],\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# Training\n",
    "acc = Accuracy()\n",
    "model.train()\n",
    "for step, batch in enumerate(progress_bar(train_dataloader)):\n",
    "    batch = to_gpu(batch)\n",
    "    with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        out = model(**batch)\n",
    "        loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset  \n",
    "    if step%config.gradient_accumulation_steps == 0:\n",
    "        # we can log the metrics to W&B\n",
    "        wandb.log({\"loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "                   \"accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
    "                   \"lr\": scheduler.get_last_lr()[0]})\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    # we perform validation every now and then\n",
    "    if step%config.eval_every==0 or step%(config.epoch_sz-1)==0:\n",
    "        validate()\n",
    "        \n",
    "# we save the model checkpoint at the end\n",
    "save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\", log=config.log_model)\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec95b95-3e81-4a40-8eea-34048c992ba9",
   "metadata": {},
   "source": [
    "This trains in around 80 minutes on a A100. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fafb9-b41f-401b-a1c4-37e1ede2e639",
   "metadata": {},
   "source": [
    "Let's log a table with model predictions on the eval_dataset (or at least the 250 first samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89717a-ac70-43e8-9b7c-edd8d2c54cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           job_type=\"eval\",\n",
    "           config=config): # the Hyperparameters I want to keep track of\n",
    "    model.eval();\n",
    "    prompt_table(eval_dataset[:250], log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e48b3-cf27-4542-8488-9e230002eb76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
